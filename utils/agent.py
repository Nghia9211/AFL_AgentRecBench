import os
import time
import argparse
import json
import torch
from tqdm import tqdm
import random
from torch.utils.data import Dataset, DataLoader
import multiprocessing
import sys
import pandas as pd
import numpy as np
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import model

sys.modules['SASRecModules_ori'] = model

from utils.regular_function import split_user_response, split_rec_reponse
from utils.rw_process import append_jsonl, write_jsonl, read_jsonl
from utils.api_request import api_request
from utils.model import SASRec

class RecAgent:
    def __init__(self, args, mode='prior_rec'):
        self.memory = []
        self.info_list = []
        self.args = args
        self.mode = mode
        self.load_prompt()

    def load_prompt(self):
        if self.mode =='prior_rec':
            if 'amazon' in self.args.data_dir:
                from constant.amazon_prior_model_prompt import rec_system_prompt, rec_user_prompt, rec_memory_system_prompt, rec_memory_user_prompt, rec_build_memory
            elif 'goodreads' in self.args.data_dir:
                from constant.goodreads_prior_model_prompt import rec_system_prompt, rec_user_prompt, rec_memory_system_prompt, rec_memory_user_prompt, rec_build_memory
            elif 'yelp' in self.args.data_dir:
                from constant.yelp_prior_model_prompt import rec_system_prompt, rec_user_prompt, rec_memory_system_prompt, rec_memory_user_prompt, rec_build_memory
            else:
                raise ValueError("Invalid mode: {}".format(self.args.data_dir))
            self.rec_system_prompt = rec_system_prompt
            self.rec_user_prompt = rec_user_prompt
            self.rec_memory_system_prompt = rec_memory_system_prompt
            self.rec_memory_user_prompt = rec_memory_user_prompt
            self.rec_build_memory = rec_build_memory
        else:
            raise ValueError("Invalid mode: {}".format(self.mode))

    def act(self, data, reason=None, item=None):
        if self.mode =='prior_rec':
            try:
                if len(self.memory) == 0:
                    system_prompt = self.rec_system_prompt
                    user_prompt = self.rec_user_prompt.format(data['seq_str'], data['len_cans'],data['cans_str'], data['prior_answer'])
                else:
                    system_prompt = self.rec_memory_system_prompt
                    user_prompt = self.rec_memory_user_prompt.format(data['seq_str'],data['len_cans'], data['cans_str'], '\n'.join(self.memory))
                response = api_request(system_prompt, user_prompt, self.args)
                print(f"Response : {response} ")
                return response
            except Exception as e:
                print(f"LỖI KHI GỌI MODEL TRONG REC_AGENT (User ID: {data.get('id', 'N/A')}): {e}") 
                import traceback
                traceback.print_exc()
                return None 
        else:
            raise ValueError("Invalid mode: {}".format(self.mode))
        
    def build_memory(self, info):
        # Dùng item_list thay cho rec_item trong memory
        rec_item_str = ', '.join(info['rec_item_list']) if isinstance(info.get('rec_item_list'), list) else info.get('rec_item') 
        return self.rec_build_memory.format(info['epoch'], rec_item_str, info['rec_reason'], info['user_reason'])
    
    def update_memory(self, info):
        self.info_list.append(info)
        self.memory.append(self.build_memory(info))

    def save_memory(self, path):
        write_jsonl(path, self.info_list)
    
    def load_memory(self, path):
        self.info_list = read_jsonl(path)
        self.memory = [self.build_memory(info) for info in self.info_list]


class UserModelAgent:
    def __init__(self, args, mode='prior_rec'):
        self.memory = []
        self.info_list = []
        self.args = args
        self.mode = mode
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.load_prompt()
        self.load_model()
        self.id2name = dict()
        self.name2id = dict()
        self.build_id2name()
        

    def build_id2name(self):
        if any(x in self.args.data_dir for x in [ 'yelp', 'amazon', 'goodreads']):
            item_path = os.path.join(self.args.data_dir, 'id2name.txt')
            with open(item_path, 'r', encoding='utf-8') as f:
                for l in f.readlines():
                    ll = l.strip('\n').split('::')
                    self.id2name[int(ll[0])] = ll[1].strip()
                    self.name2id[ll[1].strip()] = int(ll[0])
        else:
            raise ValueError("Invalid data dir: {}".format(self.args.data_dir))
    def load_model(self):
        print(f"Loading model from {self.args.model_path}")
        data_directory = self.args.data_dir
        data_statis = pd.read_pickle(os.path.join(data_directory, 'data_statis.df'))
        self.seq_size = data_statis['seq_size'][0]
        self.item_num = data_statis['item_num'][0]
        
        # 1. Khởi tạo đối tượng model đúng class
        self.model = SASRec(64, self.item_num, self.seq_size, 0.1, self.device)
        self.model.to(self.device)

        # 2. Load dữ liệu từ file .pt
        checkpoint = torch.load(self.args.model_path, map_location=self.device, weights_only=False)

        # 3. Kiểm tra xem file lưu là nguyên state_dict hay là dict chứa nhiều thông tin (checkpoint)
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                # Trường hợp load từ file "best_model.pt"
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                # Trường hợp load từ file "last_model.pt" hoặc state_dict thuần
                self.model.load_state_dict(checkpoint)
        else:
            # Trường hợp nếu bạn lưu toàn bộ object (không khuyến khích nhưng để phòng ngừa)
            self.model = checkpoint

        self.model.eval() # Quan trọng: Chuyển sang chế độ inference
        print("Load model weights success!")
    
    def load_prompt(self):
        if self.mode == 'prior_rec':
            if 'amazon' in self.args.data_dir:
                from constant.amazon_prior_model_prompt import user_system_prompt,user_user_prompt, user_memory_system_prompt, user_memory_user_prompt, user_build_memory, user_build_memory_2
            elif 'goodreads' in self.args.data_dir:
                from constant.goodreads_prior_model_prompt import user_system_prompt,user_user_prompt, user_memory_system_prompt, user_memory_user_prompt, user_build_memory, user_build_memory_2
            elif 'yelp' in self.args.data_dir:
                from constant.yelp_prior_model_prompt import user_system_prompt,user_user_prompt, user_memory_system_prompt, user_memory_user_prompt, user_build_memory, user_build_memory_2
            else:
                raise ValueError("Invalid dataset: {}".format(self.args.data_dir))
            self.user_system_prompt = user_system_prompt
            self.user_user_prompt = user_user_prompt
            self.user_memory_system_prompt = user_memory_system_prompt
            self.user_memory_user_prompt = user_memory_user_prompt
            self.user_build_memory = user_build_memory
            self.user_build_memory_2 = user_build_memory_2

    def act(self, data, reason=None, item_list=None): # <--- ĐÃ SỬA
            if self.mode == 'prior_rec':
                model_output = self.model_generate(data['seq'], data['len_seq'], data['cans'])
                
                rec_list_str = ', '.join(item_list) if item_list else "None" 

                if len(self.memory) == 0:
                    system_prompt = self.user_system_prompt.format(data['seq_str'], data['prior_answer'])
                    user_prompt = self.user_user_prompt.format(data['cans_str'], model_output, rec_list_str, reason) # <--- ĐÃ SỬA
                else:
                    # Subsequent rounds: Use memory prompts
                    system_prompt = self.user_memory_system_prompt.format(data['seq_str'], data['prior_answer'])
                    # SỬA DÒNG NÀY: Truyền rec_list_str thay cho item cũ
                    user_prompt = self.user_memory_user_prompt.format(data['cans_str'], model_output, '\n'.join(self.memory), rec_list_str, reason) # <--- ĐÃ SỬA
                

                response = api_request(system_prompt, user_prompt, self.args)
                return response
            else:
                raise ValueError("Invalid mode: {}".format(self.mode))

    def pred_model(self, data, score):
        if len(self.memory) == 0:
            system_prompt = self.user_system_prompt.format(data['seq_str'])
            user_prompt = self.user_user_prompt.format(data['pred_item'], score)
        else:
            system_prompt = self.user_memory_system_prompt.format(data['seq_str'], data['cans_str'],'\n'.join(self.memory))
            user_prompt = self.user_memory_user_prompt.format(data['pred_item'],score)
            
        response = api_request(system_prompt, user_prompt, self.args)
        return response
    
    def build_memory(self, info):
        if info['user_reason'] is not None:
            return self.user_build_memory.format(info['epoch'], info['rec_item_list'], info['rec_reason'], info['user_reason'])
        else:
            return self.user_build_memory_2.format(info['epoch'], info['rec_item_list'], info['rec_reason'])
    
    def update_memory(self, info):
        self.info_list.append(info)
        self.memory.append(self.build_memory(info))

    def save_memory(self, path):
        write_jsonl(path, self.info_list)
    
    def load_memory(self, path):
        self.info_list = read_jsonl(path)
        self.memory = [self.build_memory(info) for info in self.info_list]

    def model_generate(self, seq, len_seq, candidates):
        seq_b = [seq]
        len_seq_b = [len_seq]
        states = np.array(seq_b)
        states = torch.LongTensor(states)
        states = states.to(self.device)
        prediction = self.model.forward_eval(states, np.array(len_seq_b))

        sampling_idx=[True]*self.item_num
        cans_num = len(candidates)
        for i in candidates:
            sampling_idx.__setitem__(i,False)
        sampling_idxs = [torch.tensor(sampling_idx)]
        sampling_idxs=torch.stack(sampling_idxs,dim=0)
        prediction = prediction.cpu().detach().masked_fill(sampling_idxs,prediction.min().item()-1)
        values, topK = prediction.topk(cans_num, dim=1, largest=True, sorted=True)
        topK = topK.numpy()[0]
        name_list = [self.id2name[id] for id in topK]
        len_ret = int(len(name_list) /4 )
        return ', '.join(name_list[:len_ret])

    def score(self, seq, len_seq, candidates):
        #print("seq = ", seq)
        #print("len seq = ", len_seq)
        #print("cans = ", candidates)
        seq_b = [seq]
        len_seq_b = [len_seq]
        states = np.array(seq_b)
        states = torch.LongTensor(states)
        states = states.to(self.device)
        # pred
        prediction = self.model.forward_eval(states, np.array(len_seq_b))

        sampling_idx=[True]*self.item_num
        cans_num = len(candidates)
        for i in candidates:
            sampling_idx.__setitem__(i,False)
        sampling_idxs = [torch.tensor(sampling_idx)]
        sampling_idxs=torch.stack(sampling_idxs,dim=0)
        prediction = prediction.cpu().detach().masked_fill(sampling_idxs,prediction.min().item()-1)
        values, topK = prediction.topk(cans_num, dim=1, largest=True, sorted=True)
        values = values.numpy()[0]
        topK = topK.numpy()[0]
        score_dict = {}
        for i in range(len(topK)):
            id = topK[i]
            score = values[i]
            name = self.id2name[id]
            score_dict[name] = score
        return score_dict